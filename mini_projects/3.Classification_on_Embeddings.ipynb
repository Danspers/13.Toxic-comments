{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "045a5103-bb0f-4fb7-835a-238e053b2157"
    ]
   },
   "source": [
    "# Классификация на эмбеддингах"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задача:** Обучить модель логистической регрессии на эмбеддингах. Напечатайть на экране значение accuracy на обучающей выборке.\n",
    "\n",
    "Чтобы не создавать эмбеддинги слишком долго, отобрать из выборки только 400 случайных элементов. Для корректного тестирования поделите их на обучающую и тестовую выборки в соотношении 50:50.\n",
    "Целевой признак находится в переменной `df_tweets['positive']`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# импорт основных библиотек\n",
    "import torch\n",
    "import transformers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# импорт спец. элементов\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# чтение файла\n",
    "df_tweets = pd.read_csv('/datasets/tweets.csv')\n",
    "\n",
    "# выдержка из 400 случайных объектов\n",
    "df_tweets = df_tweets.sample(400).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# инициализация токенизатора\n",
    "tokenizer = transformers.BertTokenizer(vocab_file='/datasets/ds_bert/vocab.txt')\n",
    "\n",
    "# токенизация\n",
    "tokenized = df_tweets['text'].apply(lambda x: tokenizer.encode(x, add_special_tokens=True))\n",
    "\n",
    "# поиск макс. длины токена\n",
    "max_len = 0\n",
    "for i in tokenized.values:\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)\n",
    "\n",
    "# подгон токенов под одну длинну\n",
    "padded = np.array([i + [0]*(max_len - len(i)) for i in tokenized.values])\n",
    "\n",
    "# создание маски\n",
    "attention_mask = np.where(padded != 0, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Инициализируем конфигурацию _BertConfig_. В качестве аргумента передадим ей JSON-файл с описанием настроек модели. JSON (англ. JavaScript Object Notation, «объектная запись JavaScript») — это организованный по ключам поток цифр, букв, двоеточий и фигурных скобок, который возвращает сервер при запросе."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /datasets/ds_bert/rubert_model.bin were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# инициализируем конфигурацию BertConfig\n",
    "config = transformers.BertConfig.from_json_file('/datasets/ds_bert/bert_config.json')\n",
    "\n",
    "# инициализируем саму модель класса BertModel + файл с предобученной моделью и конфигурацией\n",
    "model = transformers.BertModel.from_pretrained('/datasets/ds_bert/rubert_model.bin', config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Эмбеддинги модель _BERT_ создаёт батчами. Чтобы хватило оперативной памяти, сделаем размер батча = 100.\n",
    "\n",
    "Преобразуем данные в формат тензоров (англ. tensor) — многомерных векторов в библиотеке torch. Тип данных _LongTensor_ (англ. «длинный тензор») хранит числа в «длинном формате», то есть выделяет на каждое число 64 бита.\n",
    "\n",
    "Чтобы получить эмбеддинги для батча, передадим модели данные и маску `attention_mask_batch`.\n",
    "\n",
    "Для ускорения вычисления функцией `no_grad()` в библиотеке **torch** укажем, что градиенты не нужны: модель _BERT_ обучать не будем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de14466bcf324df3bd9f4555d5ca9a95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# кодирование слов в векторы (энбеддинги)\n",
    "batch_size = 100\n",
    "embeddings = []\n",
    "\n",
    "for i in tqdm(range(padded.shape[0] // batch_size)):\n",
    "        batch = torch.LongTensor(padded[batch_size*i:batch_size*(i+1)]) \n",
    "        attention_mask_batch = torch.LongTensor(attention_mask[batch_size*i:batch_size*(i+1)])\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            batch_embeddings = model(batch, attention_mask=attention_mask_batch)\n",
    "        \n",
    "        embeddings.append(batch_embeddings[0][:,0,:].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Соберём все эмбеддинги в матрицу признаков вызовом функции `concatenate()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# выделение признаков\n",
    "features = np.concatenate(embeddings)\n",
    "target = df_tweets['positive']\n",
    "\n",
    "# обучение и протестирование модель\n",
    "LR_model = LogisticRegression()\n",
    "LR_model.fit(features, target)\n",
    "prediction = LR_model.predict(features)\n",
    "print(accuracy_score(target, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>758</th>\n",
       "      <th>759</th>\n",
       "      <th>760</th>\n",
       "      <th>761</th>\n",
       "      <th>762</th>\n",
       "      <th>763</th>\n",
       "      <th>764</th>\n",
       "      <th>765</th>\n",
       "      <th>766</th>\n",
       "      <th>767</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.253247</td>\n",
       "      <td>-0.219286</td>\n",
       "      <td>-1.608447</td>\n",
       "      <td>-0.502738</td>\n",
       "      <td>0.725892</td>\n",
       "      <td>0.245856</td>\n",
       "      <td>-0.220571</td>\n",
       "      <td>0.144129</td>\n",
       "      <td>0.218670</td>\n",
       "      <td>-0.914865</td>\n",
       "      <td>...</td>\n",
       "      <td>0.303589</td>\n",
       "      <td>0.092892</td>\n",
       "      <td>-0.837715</td>\n",
       "      <td>-0.152068</td>\n",
       "      <td>-0.508317</td>\n",
       "      <td>0.193780</td>\n",
       "      <td>0.232059</td>\n",
       "      <td>-0.492279</td>\n",
       "      <td>1.024564</td>\n",
       "      <td>-0.411419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.233973</td>\n",
       "      <td>0.228894</td>\n",
       "      <td>-0.788425</td>\n",
       "      <td>-0.680701</td>\n",
       "      <td>0.537587</td>\n",
       "      <td>0.969870</td>\n",
       "      <td>-0.081464</td>\n",
       "      <td>0.394526</td>\n",
       "      <td>0.076317</td>\n",
       "      <td>0.116301</td>\n",
       "      <td>...</td>\n",
       "      <td>0.945812</td>\n",
       "      <td>-0.374136</td>\n",
       "      <td>-0.435543</td>\n",
       "      <td>-0.393317</td>\n",
       "      <td>-0.130384</td>\n",
       "      <td>-1.257408</td>\n",
       "      <td>-0.835614</td>\n",
       "      <td>-0.602395</td>\n",
       "      <td>0.494750</td>\n",
       "      <td>-0.903654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.082147</td>\n",
       "      <td>0.442392</td>\n",
       "      <td>-0.220233</td>\n",
       "      <td>-0.337405</td>\n",
       "      <td>0.514747</td>\n",
       "      <td>-0.049011</td>\n",
       "      <td>-0.268052</td>\n",
       "      <td>-0.078710</td>\n",
       "      <td>0.076074</td>\n",
       "      <td>0.290851</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.024144</td>\n",
       "      <td>0.115906</td>\n",
       "      <td>-1.464156</td>\n",
       "      <td>-0.638169</td>\n",
       "      <td>0.176227</td>\n",
       "      <td>-0.237281</td>\n",
       "      <td>-0.016916</td>\n",
       "      <td>0.300110</td>\n",
       "      <td>1.012946</td>\n",
       "      <td>-0.370803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.884442</td>\n",
       "      <td>-0.611157</td>\n",
       "      <td>-0.552049</td>\n",
       "      <td>-0.646139</td>\n",
       "      <td>0.612808</td>\n",
       "      <td>0.250996</td>\n",
       "      <td>-0.445902</td>\n",
       "      <td>0.698740</td>\n",
       "      <td>0.267339</td>\n",
       "      <td>0.180597</td>\n",
       "      <td>...</td>\n",
       "      <td>0.400547</td>\n",
       "      <td>1.293199</td>\n",
       "      <td>-1.793786</td>\n",
       "      <td>-1.178009</td>\n",
       "      <td>0.280816</td>\n",
       "      <td>-0.385244</td>\n",
       "      <td>-0.009727</td>\n",
       "      <td>-1.077435</td>\n",
       "      <td>0.979432</td>\n",
       "      <td>-1.021986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.469277</td>\n",
       "      <td>-0.231216</td>\n",
       "      <td>-0.678387</td>\n",
       "      <td>0.047896</td>\n",
       "      <td>0.702712</td>\n",
       "      <td>0.460773</td>\n",
       "      <td>0.433854</td>\n",
       "      <td>-0.117043</td>\n",
       "      <td>0.168424</td>\n",
       "      <td>-0.219428</td>\n",
       "      <td>...</td>\n",
       "      <td>0.433907</td>\n",
       "      <td>0.351283</td>\n",
       "      <td>-0.542122</td>\n",
       "      <td>-0.533007</td>\n",
       "      <td>0.223880</td>\n",
       "      <td>0.262744</td>\n",
       "      <td>0.009598</td>\n",
       "      <td>0.017871</td>\n",
       "      <td>0.513239</td>\n",
       "      <td>-0.188388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>0.023489</td>\n",
       "      <td>0.009346</td>\n",
       "      <td>-0.728182</td>\n",
       "      <td>0.085365</td>\n",
       "      <td>0.021931</td>\n",
       "      <td>0.255060</td>\n",
       "      <td>-0.047340</td>\n",
       "      <td>-0.094749</td>\n",
       "      <td>0.247499</td>\n",
       "      <td>0.028535</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.045042</td>\n",
       "      <td>-0.016276</td>\n",
       "      <td>-0.386826</td>\n",
       "      <td>-0.004383</td>\n",
       "      <td>-0.183166</td>\n",
       "      <td>-0.237796</td>\n",
       "      <td>-1.104481</td>\n",
       "      <td>-0.016082</td>\n",
       "      <td>0.625905</td>\n",
       "      <td>-0.177017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>0.037798</td>\n",
       "      <td>-0.023409</td>\n",
       "      <td>-0.508086</td>\n",
       "      <td>-0.082595</td>\n",
       "      <td>0.517871</td>\n",
       "      <td>0.254651</td>\n",
       "      <td>-0.199214</td>\n",
       "      <td>0.004147</td>\n",
       "      <td>0.518994</td>\n",
       "      <td>-0.020038</td>\n",
       "      <td>...</td>\n",
       "      <td>0.076869</td>\n",
       "      <td>-0.470810</td>\n",
       "      <td>-0.617664</td>\n",
       "      <td>0.140889</td>\n",
       "      <td>0.353920</td>\n",
       "      <td>-0.506663</td>\n",
       "      <td>-1.058230</td>\n",
       "      <td>0.026615</td>\n",
       "      <td>0.400752</td>\n",
       "      <td>-0.025611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>-1.397274</td>\n",
       "      <td>-0.383809</td>\n",
       "      <td>-0.738730</td>\n",
       "      <td>0.155169</td>\n",
       "      <td>1.873098</td>\n",
       "      <td>-0.052520</td>\n",
       "      <td>-0.381069</td>\n",
       "      <td>0.653646</td>\n",
       "      <td>-0.437332</td>\n",
       "      <td>1.328871</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.253383</td>\n",
       "      <td>1.636567</td>\n",
       "      <td>-1.240876</td>\n",
       "      <td>-0.476979</td>\n",
       "      <td>0.727108</td>\n",
       "      <td>-0.812097</td>\n",
       "      <td>-0.800558</td>\n",
       "      <td>-1.010794</td>\n",
       "      <td>0.252874</td>\n",
       "      <td>-1.172189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>-1.131692</td>\n",
       "      <td>-0.225830</td>\n",
       "      <td>-0.161463</td>\n",
       "      <td>-0.879736</td>\n",
       "      <td>0.015725</td>\n",
       "      <td>1.093419</td>\n",
       "      <td>-1.050108</td>\n",
       "      <td>0.848875</td>\n",
       "      <td>0.233880</td>\n",
       "      <td>0.321800</td>\n",
       "      <td>...</td>\n",
       "      <td>0.105072</td>\n",
       "      <td>0.362218</td>\n",
       "      <td>-0.497042</td>\n",
       "      <td>-0.819721</td>\n",
       "      <td>0.225314</td>\n",
       "      <td>-0.611872</td>\n",
       "      <td>-1.117782</td>\n",
       "      <td>-0.538984</td>\n",
       "      <td>0.530190</td>\n",
       "      <td>-0.673876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>-0.278832</td>\n",
       "      <td>-1.165192</td>\n",
       "      <td>-0.307101</td>\n",
       "      <td>0.255213</td>\n",
       "      <td>-0.236414</td>\n",
       "      <td>0.700277</td>\n",
       "      <td>-0.955605</td>\n",
       "      <td>0.225052</td>\n",
       "      <td>0.024751</td>\n",
       "      <td>0.050790</td>\n",
       "      <td>...</td>\n",
       "      <td>0.552159</td>\n",
       "      <td>0.109337</td>\n",
       "      <td>-0.908472</td>\n",
       "      <td>0.046888</td>\n",
       "      <td>-0.925564</td>\n",
       "      <td>-1.073409</td>\n",
       "      <td>-1.147433</td>\n",
       "      <td>0.525305</td>\n",
       "      <td>0.669152</td>\n",
       "      <td>-0.532600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows × 768 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6    \\\n",
       "0    0.253247 -0.219286 -1.608447 -0.502738  0.725892  0.245856 -0.220571   \n",
       "1   -0.233973  0.228894 -0.788425 -0.680701  0.537587  0.969870 -0.081464   \n",
       "2   -0.082147  0.442392 -0.220233 -0.337405  0.514747 -0.049011 -0.268052   \n",
       "3   -0.884442 -0.611157 -0.552049 -0.646139  0.612808  0.250996 -0.445902   \n",
       "4    0.469277 -0.231216 -0.678387  0.047896  0.702712  0.460773  0.433854   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "395  0.023489  0.009346 -0.728182  0.085365  0.021931  0.255060 -0.047340   \n",
       "396  0.037798 -0.023409 -0.508086 -0.082595  0.517871  0.254651 -0.199214   \n",
       "397 -1.397274 -0.383809 -0.738730  0.155169  1.873098 -0.052520 -0.381069   \n",
       "398 -1.131692 -0.225830 -0.161463 -0.879736  0.015725  1.093419 -1.050108   \n",
       "399 -0.278832 -1.165192 -0.307101  0.255213 -0.236414  0.700277 -0.955605   \n",
       "\n",
       "          7         8         9    ...       758       759       760  \\\n",
       "0    0.144129  0.218670 -0.914865  ...  0.303589  0.092892 -0.837715   \n",
       "1    0.394526  0.076317  0.116301  ...  0.945812 -0.374136 -0.435543   \n",
       "2   -0.078710  0.076074  0.290851  ... -0.024144  0.115906 -1.464156   \n",
       "3    0.698740  0.267339  0.180597  ...  0.400547  1.293199 -1.793786   \n",
       "4   -0.117043  0.168424 -0.219428  ...  0.433907  0.351283 -0.542122   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "395 -0.094749  0.247499  0.028535  ... -0.045042 -0.016276 -0.386826   \n",
       "396  0.004147  0.518994 -0.020038  ...  0.076869 -0.470810 -0.617664   \n",
       "397  0.653646 -0.437332  1.328871  ... -0.253383  1.636567 -1.240876   \n",
       "398  0.848875  0.233880  0.321800  ...  0.105072  0.362218 -0.497042   \n",
       "399  0.225052  0.024751  0.050790  ...  0.552159  0.109337 -0.908472   \n",
       "\n",
       "          761       762       763       764       765       766       767  \n",
       "0   -0.152068 -0.508317  0.193780  0.232059 -0.492279  1.024564 -0.411419  \n",
       "1   -0.393317 -0.130384 -1.257408 -0.835614 -0.602395  0.494750 -0.903654  \n",
       "2   -0.638169  0.176227 -0.237281 -0.016916  0.300110  1.012946 -0.370803  \n",
       "3   -1.178009  0.280816 -0.385244 -0.009727 -1.077435  0.979432 -1.021986  \n",
       "4   -0.533007  0.223880  0.262744  0.009598  0.017871  0.513239 -0.188388  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "395 -0.004383 -0.183166 -0.237796 -1.104481 -0.016082  0.625905 -0.177017  \n",
       "396  0.140889  0.353920 -0.506663 -1.058230  0.026615  0.400752 -0.025611  \n",
       "397 -0.476979  0.727108 -0.812097 -0.800558 -1.010794  0.252874 -1.172189  \n",
       "398 -0.819721  0.225314 -0.611872 -1.117782 -0.538984  0.530190 -0.673876  \n",
       "399  0.046888 -0.925564 -1.073409 -1.147433  0.525305  0.669152 -0.532600  \n",
       "\n",
       "[400 rows x 768 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(pd.DataFrame(features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 2015,
    "start_time": "2023-03-19T19:47:15.326Z"
   },
   {
    "duration": 44,
    "start_time": "2023-03-19T19:47:26.258Z"
   },
   {
    "duration": 4136,
    "start_time": "2023-03-19T20:04:21.813Z"
   },
   {
    "duration": 34976,
    "start_time": "2023-03-19T20:04:41.142Z"
   },
   {
    "duration": 17,
    "start_time": "2023-03-19T20:06:26.589Z"
   },
   {
    "duration": 48,
    "start_time": "2023-03-19T20:20:19.655Z"
   },
   {
    "duration": 358,
    "start_time": "2023-03-19T20:20:21.141Z"
   },
   {
    "duration": 2504,
    "start_time": "2023-03-19T20:20:25.608Z"
   },
   {
    "duration": 71050,
    "start_time": "2023-03-19T20:20:32.250Z"
   },
   {
    "duration": 1141,
    "start_time": "2023-03-19T20:21:57.957Z"
   },
   {
    "duration": 4,
    "start_time": "2023-03-19T20:22:13.058Z"
   },
   {
    "duration": 879,
    "start_time": "2023-03-19T20:22:23.211Z"
   },
   {
    "duration": 57,
    "start_time": "2023-03-21T10:58:03.887Z"
   },
   {
    "duration": 2901,
    "start_time": "2023-03-21T10:58:09.300Z"
   },
   {
    "duration": 91,
    "start_time": "2023-03-21T10:58:13.678Z"
   },
   {
    "duration": 2366,
    "start_time": "2023-03-21T10:58:15.584Z"
   },
   {
    "duration": 5994,
    "start_time": "2023-03-21T10:58:19.592Z"
   },
   {
    "duration": 96702,
    "start_time": "2023-03-21T10:58:28.415Z"
   },
   {
    "duration": 15,
    "start_time": "2023-03-21T11:00:45.003Z"
   },
   {
    "duration": 286,
    "start_time": "2023-03-21T11:00:57.459Z"
   },
   {
    "duration": 23,
    "start_time": "2023-03-21T11:01:27.171Z"
   },
   {
    "duration": 19,
    "start_time": "2023-03-21T11:01:49.358Z"
   },
   {
    "duration": 10,
    "start_time": "2023-03-21T11:02:02.618Z"
   },
   {
    "duration": 17,
    "start_time": "2023-03-21T11:02:18.057Z"
   },
   {
    "duration": 8,
    "start_time": "2023-03-21T11:02:31.855Z"
   },
   {
    "duration": 5,
    "start_time": "2023-03-21T11:02:37.907Z"
   },
   {
    "duration": 1205,
    "start_time": "2023-03-21T11:02:57.692Z"
   },
   {
    "duration": 4,
    "start_time": "2023-03-21T11:03:06.157Z"
   },
   {
    "duration": 18,
    "start_time": "2023-03-21T11:03:17.646Z"
   },
   {
    "duration": 32,
    "start_time": "2023-03-21T11:03:36.939Z"
   },
   {
    "duration": 6649,
    "start_time": "2023-06-13T11:40:03.356Z"
   },
   {
    "duration": 288,
    "start_time": "2023-06-13T11:40:12.687Z"
   },
   {
    "duration": 2243,
    "start_time": "2023-06-13T11:40:15.896Z"
   },
   {
    "duration": 28294,
    "start_time": "2023-06-13T11:40:20.647Z"
   },
   {
    "duration": 76090,
    "start_time": "2023-06-13T11:42:26.800Z"
   },
   {
    "duration": 316,
    "start_time": "2023-06-13T11:46:12.619Z"
   },
   {
    "duration": 1984,
    "start_time": "2023-06-13T11:46:33.205Z"
   },
   {
    "duration": 32,
    "start_time": "2023-06-13T11:46:42.608Z"
   },
   {
    "duration": 2367,
    "start_time": "2023-06-13T11:54:18.342Z"
   },
   {
    "duration": 42,
    "start_time": "2023-06-13T11:54:20.711Z"
   },
   {
    "duration": 2032,
    "start_time": "2023-06-13T11:54:20.754Z"
   },
   {
    "duration": 2536,
    "start_time": "2023-06-13T11:54:22.788Z"
   },
   {
    "duration": 84966,
    "start_time": "2023-06-13T11:54:25.326Z"
   },
   {
    "duration": 2393,
    "start_time": "2023-06-13T11:55:50.294Z"
   },
   {
    "duration": 208,
    "start_time": "2023-06-13T11:55:52.689Z"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
