{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "90174557-ccbc-4d19-b780-5988d67a3706"
    ]
   },
   "source": [
    "# Предобработка твитов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# ПРИМЕР\n",
    "\n",
    "# инициализируем токенизатор\n",
    "tokenizer = transformers.BertTokenizer(vocab_file='/datasets/ds_bert/vocab.txt')\n",
    "\n",
    "# токенизируем текст\n",
    "vector = tokenizer.encode('Очень удобно использовать уже готовый трансформатор текста', add_special_tokens=True)\n",
    "\n",
    "# применим padding к векторам\n",
    "n = 280\n",
    "# англ. вектор с отступами\n",
    "padded = vector + [0]*(n - len(vector))\n",
    "\n",
    "# создадим маску для важных токенов\n",
    "attention_mask = np.where(padded != 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# чтение датасета\n",
    "df = pd.read_csv('/datasets/tweets.csv')\n",
    "\n",
    "# инициализируем токенизатор\n",
    "tokenizer = transformers.BertTokenizer(vocab_file='/datasets/ds_bert/vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    }
   ],
   "source": [
    "# 1ый вариант обработки датасета\n",
    "tokens_list = []\n",
    "attention_mask_list = []\n",
    "for i in range(len(df)):\n",
    "    # токенизируем текст\n",
    "    vector = tokenizer.encode(df.loc[i,'text'], add_special_tokens=True)\n",
    "    \n",
    "    # применим padding к векторам\n",
    "    n = 133\n",
    "    padded = vector + [0]*(n - len(vector))\n",
    "    \n",
    "    # создадим маску для важных токенов\n",
    "    attention_mask = np.where(padded != 0, 1, 0)\n",
    "    \n",
    "    # сохранение токена и маски важности\n",
    "    tokens_list.append(padded)\n",
    "    attention_mask_list.append(attention_mask)\n",
    "    \n",
    "print(len(attention_mask_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    }
   ],
   "source": [
    "# 2ой вариант обработки датасета\n",
    "# токенизируем текст\n",
    "tokens = df['text'].apply(lambda x: tokenizer.encode(x, add_special_tokens=True))\n",
    "\n",
    "# применим padding к векторам\n",
    "tokens_padded = tokens.apply(lambda x: x + [0]*(n - len(x)))\n",
    "\n",
    "# создадим маску для важных токенов\n",
    "attention_mask = tokens_padded.apply(lambda x: np.where(x != 0, 1, 0))    \n",
    "\n",
    "print(len(attention_mask_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 8026,
    "start_time": "2023-03-19T18:12:44.292Z"
   },
   {
    "duration": 17,
    "start_time": "2023-03-19T18:13:11.349Z"
   },
   {
    "duration": 227,
    "start_time": "2023-03-19T18:13:18.413Z"
   },
   {
    "duration": 3,
    "start_time": "2023-03-19T18:14:00.370Z"
   },
   {
    "duration": 15,
    "start_time": "2023-03-19T18:14:26.146Z"
   },
   {
    "duration": 16,
    "start_time": "2023-03-19T18:14:40.826Z"
   },
   {
    "duration": 184,
    "start_time": "2023-03-19T18:14:44.822Z"
   },
   {
    "duration": 67,
    "start_time": "2023-03-19T18:16:18.815Z"
   },
   {
    "duration": 49,
    "start_time": "2023-03-19T18:17:29.750Z"
   },
   {
    "duration": 23,
    "start_time": "2023-03-19T18:29:00.962Z"
   },
   {
    "duration": 34,
    "start_time": "2023-03-19T18:29:31.406Z"
   },
   {
    "duration": 21,
    "start_time": "2023-03-19T18:30:36.548Z"
   },
   {
    "duration": 2196,
    "start_time": "2023-03-19T18:32:01.683Z"
   },
   {
    "duration": 20,
    "start_time": "2023-03-19T18:33:32.346Z"
   },
   {
    "duration": 2104,
    "start_time": "2023-03-19T18:33:43.690Z"
   },
   {
    "duration": 18,
    "start_time": "2023-03-19T18:42:18.985Z"
   },
   {
    "duration": 2217,
    "start_time": "2023-03-19T18:42:28.184Z"
   },
   {
    "duration": 2346,
    "start_time": "2023-03-19T18:48:45.404Z"
   },
   {
    "duration": 45,
    "start_time": "2023-03-19T18:49:59.701Z"
   },
   {
    "duration": 2491,
    "start_time": "2023-03-19T18:50:05.131Z"
   },
   {
    "duration": 2274,
    "start_time": "2023-03-19T18:50:22.132Z"
   },
   {
    "duration": 4,
    "start_time": "2023-03-19T18:51:09.311Z"
   },
   {
    "duration": 2367,
    "start_time": "2023-03-19T18:52:07.467Z"
   },
   {
    "duration": 2269,
    "start_time": "2023-03-19T19:00:45.515Z"
   },
   {
    "duration": 2327,
    "start_time": "2023-03-19T19:02:43.183Z"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
